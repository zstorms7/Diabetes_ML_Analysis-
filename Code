import pandas as pd
import numpy as np
import itertools
import seaborn as sns
from io import StringIO
import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeClassifier # Import Decision Tree Classifier
from sklearn.model_selection import train_test_split # Import train_test_split function
from sklearn import metrics #Import scikit-learn metrics module for accuracy calculation
import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormap
from sklearn import datasets #Import scikit-learn dataset library
from sklearn.tree import DecisionTreeClassifier # Import Decision Tree Classifier
from sklearn.model_selection import GridSearchCV # Import for hyperparameter tuning
from sklearn.model_selection import train_test_split # Import train_test_split function
from sklearn import metrics #Import scikit-learn metrics module for accuracy calculation
from sklearn.metrics import confusion_matrix

col_names = ['pregnant', 'glucose', 'bp', 'skin', 'insulin', 'bmi', 'pedigree', 'age', 'label']
url = "https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv"
data = pd.read_csv(url, header=None, names=col_names)
print(data.head(5))

print(data.isnull().sum())

print(data.describe())


diabetes_copy = data.copy(deep=True)

diabetes_copy[['glucose', 'bp', 'skin', 'insulin', 'bmi']] = diabetes_copy[['glucose', 'bp', 'skin', 'insulin', 'bmi']].replace(0, np.nan)

print(diabetes_copy.isnull().sum())



data['bmi'] = data['bmi'].median()  # Calculate bmi median
data['glucose'] = data['glucose'].median()  # Calculate glucose median
data['bp'] = data['bp'].median()  # Calculate bp median
data['skin'] = data['skin'].median()  # Calculate skin median
data['insulin'] = data['insulin'].median()  # Calculate insulin median

print(data.isnull().sum())
print(data.describe())

feature_cols = ['pregnant', 'insulin', 'bmi', 'age','glucose','bp','pedigree']
X = data[feature_cols]
y = data[['label']]

print(X.shape)
print(y.shape)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)

dt_model = DecisionTreeClassifier(max_depth=4,min_samples_split=2)

dt_model.fit(X_train, y_train)

y_train_pred = dt_model.predict(X_train)
y_test_pred = dt_model.predict(X_test)

print(confusion_matrix(y_test, y_test_pred))

from sklearn.metrics import accuracy_score
accuracy = accuracy_score(y_test, y_test_pred)

print(f"Model Accuracy on Test Set: {accuracy:.4f}")

from sklearn.tree import export_graphviz
from six import StringIO
from IPython.display import Image
import pydotplus

dot_data = StringIO()
export_graphviz(dt_model,
                out_file=dot_data,
                filled=True, rounded=True,
                special_characters=True,feature_names = feature_cols,class_names=['0','1'])
graph = pydotplus.graph_from_dot_data(dot_data.getvalue())
graph.write_png('diabetes.png')
Image(graph.create_png())

dt2 = DecisionTreeClassifier(criterion="entropy", max_depth=3,
                             random_state = 1)

dt2 = dt2.fit(X_train,y_train)

y_pred = dt2.predict(X_test)

print("Accuracy:",metrics.accuracy_score(y_test, y_pred))

dot_data = StringIO()
export_graphviz(dt2,
                out_file=dot_data,
                filled=True, rounded=True,
                special_characters=True, feature_names = feature_cols,class_names=['0','1'])
graph = pydotplus.graph_from_dot_data(dot_data.getvalue())
graph.write_png('diabetes.png')
Image(graph.create_png())


dt3 = DecisionTreeClassifier(criterion="entropy", max_depth=4, min_samples_split=3,
                             random_state = 1)

dt3 = dt3.fit(X_train,y_train)

y_pred = dt3.predict(X_test)

print("Accuracy:",metrics.accuracy_score(y_test, y_pred))


dt4 = DecisionTreeClassifier(criterion="entropy", max_depth=3, min_samples_split=2, random_state=1)

dt4 = dt4.fit(X_train,y_train)

y_pred = dt4.predict(X_test)

print("Accuracy:",metrics.accuracy_score(y_test, y_pred))

model = DecisionTreeClassifier(random_state = 42)

param_grid = {'criterion': ['gini', 'entropy'],
              'max_depth': [3, 4, 5, 6, 7, 8, 9],
              'min_samples_split': [2, 3, 4, 5, 6, 7, 8, 9],
              'min_samples_leaf': [1, 2, 3, 4, 5, 6, 7, 8, 9]}

grid = GridSearchCV(model, param_grid, cv=5)
grid.fit(X_train, y_train)

print("Best parameters:", grid.best_params_)
print("Best score:", grid.best_score_)

best_model = grid.best_estimator_
print("Accuracy of best decision tree: ", best_model.score(X_test, y_test))


from sklearn.ensemble import RandomForestClassifier

rf=RandomForestClassifier(n_estimators=100, random_state=1)

rf.fit(X_train,y_train)

y_pred=rf.predict(X_test)

print("Accuracy:",metrics.accuracy_score(y_test, y_pred))

feature_order = ['pregnant', 'insulin', 'bmi', 'age', 'glucose', 'bp', 'pedigree']

input_data = pd.DataFrame([[0, 133, 33, 31, 115, 81, 0.3]], columns=feature_order)

print(rf.predict(input_data))

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split

X = data.drop(columns=['label'])
y = data['label']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)

importance_df = pd.DataFrame({
    'Feature': X.columns,
    'Importance': rf.feature_importances_
}).sort_values(by='Importance', ascending=False)






%matplotlib inline

plt.figure(figsize=(10, 6))
sns.barplot(x=importance_df['Importance'], y=importance_df['Feature'], palette='coolwarm')

plt.xlabel('Feature Importance Score')
plt.ylabel('Features')
plt.title("Visualizing Important Features")

plt.show()


feature_cols_2 = ['pregnant', 'age','pedigree']
X = diabetes_copy[feature_cols_2]
y = diabetes_copy.label
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=1)

rf2=RandomForestClassifier(n_estimators=100, random_state=1)

rf2.fit(X_train,y_train)

y_pred=rf2.predict(X_test)

print("Accuracy:",metrics.accuracy_score(y_test, y_pred))

from sklearn.ensemble import AdaBoostClassifier

feature_cols = ['pregnant', 'insulin', 'bmi', 'age','glucose','bp','pedigree']
X = data[feature_cols]
y = data.label

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)

abc = AdaBoostClassifier(n_estimators=50,
                         learning_rate=1, random_state=1)
model = abc.fit(X_train, y_train)

y_pred = model.predict(X_test)

print("Accuracy:",metrics.accuracy_score(y_test, y_pred))
